{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MNIST(forward propaganda).ipynb","provenance":[],"authorship_tag":"ABX9TyOqc3Pij8bvrbk+eHTzY0vo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ChoCHLpMlYyb"},"outputs":[],"source":["import numpy as np\n","from datetime import datetime\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline\n","\n","# 수치미분 함수\n","\n","def numerical_derivative(f, x):\n","    delta_x = 1e-4 # 0.0001\n","    grad = np.zeros_like(x)\n","    \n","    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n","    \n","    while not it.finished:\n","        idx = it.multi_index        \n","        tmp_val = x[idx]\n","        x[idx] = float(tmp_val) + delta_x\n","        fx1 = f(x) # f(x+delta_x)\n","        \n","        x[idx] = tmp_val - delta_x \n","        fx2 = f(x) # f(x-delta_x)\n","        grad[idx] = (fx1 - fx2) / (2*delta_x)\n","        \n","        x[idx] = tmp_val \n","        it.iternext()   \n","        \n","    return grad\n","\n","# sigmoid 함수\n","\n","def sigmoid(x):\n","    return 1 / (1+np.exp(-x))"]},{"cell_type":"code","source":["# MNIST_Test Class\n","\n","class MNIST_Test:\n","    \n","    # 생성자\n","    # xdata, tdata => numpy.array(...)\n","    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n","        \n","        self.input_nodes = input_nodes\n","        self.hidden_nodes = hidden_nodes\n","        self.output_nodes = output_nodes\n","        \n","        # 은닉층 가중치  W2  Xavier/He 방법으로 self.W2 가중치 초기화\n","        self.W2 = np.random.randn(self.input_nodes, self.hidden_nodes) / np.sqrt(self.input_nodes/2)\n","        self.b2 = np.random.rand(self.hidden_nodes)      \n","        \n","        # 출력층 가중치는 W3  Xavier/He 방법으로 self.W3 가중치 초기화\n","        self.W3 = np.random.randn(self.hidden_nodes, self.output_nodes) / np.sqrt(self.hidden_nodes/2)\n","        self.b3 = np.random.rand(self.output_nodes)      \n","        \n","        # 2층 hidden layer unit \n","        # 가중치 W, 바이어스 b 초기화\n","        #self.W2 = np.random.rand(input_nodes, hidden_nodes)  \n","        #self.b2 = np.random.rand(hidden_nodes)\n","        \n","        # 3층 output layer unit : 1 개 \n","        #self.W3 = np.random.rand(hidden_nodes,output_nodes)\n","        #self.b3 = np.random.rand(output_nodes)\n","                        \n","        # 학습률 learning rate 초기화\n","        self.learning_rate = learning_rate\n","        \n","        print(\"MNIST_Test object is created !!!\")\n","        \n","    # 손실함수\n","    def feed_forward(self):\n","        \n","        delta = 1e-7    # log 무한대 발산 방지\n","    \n","        z1 = np.dot(self.input_data, self.W2) + self.b2\n","        y1 = sigmoid(z1)\n","        \n","        z2 = np.dot(y1, self.W3) + self.b3\n","        y = sigmoid(z2)\n","    \n","        # cross-entropy \n","        return  -np.sum( self.target_data*np.log(y + delta) + (1-self.target_data)*np.log((1 - y)+delta ) )\n","    \n","    # obtain W and b\n","    def get_W_b(self):\n","        \n","        return self.W2,  self.b2, self.W3, self.b3\n","    \n","    # 손실 값 계산\n","    def loss_val(self):\n","        \n","        delta = 1e-7    # log 무한대 발산 방지\n","    \n","        z1 = np.dot(self.input_data, self.W2) + self.b2\n","        y1 = sigmoid(z1)\n","        \n","        z2 = np.dot(y1, self.W3) + self.b3\n","        y = sigmoid(z2)\n","    \n","        # cross-entropy \n","        return  -np.sum( self.target_data*np.log(y + delta) + (1-self.target_data)*np.log((1 - y)+delta ) )\n","    \n","    # query, 즉 미래 값 예측 함수\n","    def predict(self, input_data):    \n","        \n","        z2 = np.dot(input_data, self.W2) + self.b2\n","        a2 = sigmoid(z2)\n","        \n","        z3 = np.dot(a2, self.W3) + self.b3\n","        y = a3 = sigmoid(z3)\n","    \n","        # MNIST 경우는 one-hot encoding 을 적용하기 때문에\n","        # 0 또는 1 이 아닌 argmax() 를 통해 최대 인덱스를 넘겨주어야 함\n","        predicted_num = np.argmax(y)\n","    \n","        return predicted_num\n","\n","    # 정확도 측정함수\n","    def accuracy(self, input_data, target_data):\n","        \n","        matched_list = []\n","        not_matched_list = []\n","        \n","        # list which contains (index, label, prediction) value\n","        index_label_prediction_list = []\n","        \n","        # temp list which contains label and prediction in sequence\n","        temp_list = []\n","        \n","        for index in range(len(input_data)):\n","                        \n","            label = int(target_data[index])\n","                        \n","            # normalize\n","            data = (input_data[index, :] / 255.0 * 0.99) + 0.01\n","      \n","            predicted_num = self.predict(data)\n","        \n","            if label == predicted_num:\n","                matched_list.append(index)\n","                \n","            else:\n","                not_matched_list.append(index)\n","                \n","                temp_list.append(index)\n","                temp_list.append(label)\n","                temp_list.append(predicted_num)\n","                \n","                index_label_prediction_list.append(temp_list)\n","                \n","                temp_list = []\n","                \n","        print(\"Current Accuracy = \", len(matched_list)/(len(input_data)) )\n","        \n","        return matched_list, not_matched_list, index_label_prediction_list\n","    \n","        \n","    # 수치미분을 이용하여 손실함수가 최소가 될때 까지 학습하는 함수\n","    def train(self, input_data, target_data):\n","        \n","        self.input_data = input_data\n","        self.target_data = target_data\n","        \n","        f = lambda x : self.feed_forward()\n","        \n","        self.W2 -= self.learning_rate * numerical_derivative(f, self.W2)\n","    \n","        self.b2 -= self.learning_rate * numerical_derivative(f, self.b2)\n","        \n","        self.W3 -= self.learning_rate * numerical_derivative(f, self.W3)\n","    \n","        self.b3 -= self.learning_rate * numerical_derivative(f, self.b3)"],"metadata":{"id":"FNuowxr43TQ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# training data \n","training_data = np.loadtxt('./mnist_train.csv', delimiter=',', dtype=np.float32)\n","\n","print(\"training_data.shape = \", training_data.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":136},"id":"aiJkJowS3Vuo","executionInfo":{"status":"error","timestamp":1647539488949,"user_tz":-540,"elapsed":3,"user":{"displayName":"방지훈","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12258825655434514560"}},"outputId":"015072a8-e69a-4562-aee1-c8b76bd24657"},"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-7fbf529e517c>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    training_data = np.loadtxt('C:\\Users\\user\\Desktop\\trainingset\\mnist_train.csv', delimiter=',', dtype=np.float32)\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"]}]},{"cell_type":"code","source":["#hyper-parameter\n","i_nodes = training_data.shape[1] - 1    # input nodes 개수\n","h1_nodes = 8  # hidden nodes 개수. Test\n","o_nodes = 10    # output nodes 개수\n","lr = 1e-2      # learning rate\n","epochs = 1   # 반복횟수\n","\n","# 손실함수 값을 저장할 list 생성\n","loss_val_list = []\n","\n","# MNIST_Test 객체 생성\n","obj = MNIST_Test(i_nodes, h1_nodes, o_nodes, lr)\n","\n","print(\"Neural Network Learning using Numerical Derivative...\")\n","\n","start_time = datetime.now()\n","\n","for step in range(epochs):\n","    \n","    for index in range(len(training_data)):    \n","                \n","        # input_data, target_data normalize    \n","        input_data = ((training_data[index, 1:] / 255.0) * 0.99) + 0.01\n","        \n","        target_data = np.zeros(o_nodes) + 0.01    \n","        target_data[int(training_data[index, 0])] = 0.99\n","        \n","        obj.train(input_data, target_data)\n","        \n","        if (index % 200 == 0):\n","            print(\"epochs = \", step, \", index = \", index, \", loss value = \", obj.loss_val())\n","            \n","        # 손실함수 값 저장\n","        loss_val_list.append(obj.loss_val())        \n","\n","end_time = datetime.now()\n","        \n","print(\"\")\n","print(\"Elapsed Time => \", end_time - start_time)"],"metadata":{"id":"9LwO5NVd3XO4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_data = np.loadtxt('/content/drive/MyDrive/machine learning practice/trainingset/mnist_train.csv', delimiter=',', dtype=np.float32)\n","print(\"test_data.shape = \", test_data.shape)\n","\n","test_input_data = test_data[ :, 1: ]\n","test_target_data = test_data[ :, 0 ]\n","\n","(true_list_1, false_list_1, index_label_prediction_list) = obj.accuracy(test_input_data, test_target_data) "],"metadata":{"id":"OZ8Q5u-g3toi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 손실함수 추세 확인\n","x_data_list = [ index for index in range(len(training_data)) ]\n","Y_DATA_LIST = []\n","\n","for index in range(0, len(loss_val_list), 500):\n","    Y_DATA_LIST.append(loss_val_list[index])\n","    \n","plt.title('MNIST Loss Value Trend')\n","plt.xlabel('data index ( X 500)')\n","plt.ylabel('loss value')\n","plt.grid()\n","#plt.ylim(2.1, 7.1)\n","#plt.plot(x_data_list, loss_val_list, color='b')\n","plt.plot(Y_DATA_LIST, color='b')\n","plt.show()"],"metadata":{"id":"Ku38RnKe3uAD"},"execution_count":null,"outputs":[]}]}